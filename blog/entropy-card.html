<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>entropy-card</title>
  <style>
    html {
      font-family: sans-serif;
      line-height: 1.6;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 650px;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <style>
  table {
    display: table;
    margin: auto;
    width: auto;
  }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="the-entropic-framework-for-cardinality-bounds">The Entropic
Framework for Cardinality Bounds</h1>
<p>One of the most exciting developments in database theory in recent
years is the entropic framework for cardinality bounds, which emerges
from deep connections between database theory and information theory. In
this blog post, I explain the fundamental concepts and key steps for
estimating join size via information inequalities. This post is based on
a lecture by Dan Suciu during the Simons Institute program <a
href="https://simons.berkeley.edu/programs/logic-algorithms-database-theory-ai">Logic
and Algorithms in Database Theory and AI</a>. The technique is developed
in a line of work including <a
href="https://arxiv.org/abs/1711.03860">AGM13</a>, <a
href="https://theory.stanford.edu/~valiant/papers/GLV_pods.pdf">GLVV12</a>,
<a href="https://arxiv.org/pdf/1111.1109.pdf">GM14</a>, <a
href="https://arxiv.org/abs/1604.00111">ANS16</a>, <a
href="https://arxiv.org/abs/1612.02503">ANS17</a>. See <a
href="https://arxiv.org/abs/2304.11996">Suc23</a> for a wonderful survey
of the literature.</p>
<p>The intuition behind using information theory to estimate join output
size is that the <em>entropy of a relation</em>, a notion to be made
precise later, carries information about the relation. Inequalities over
entropic functions have also been studied for decades, so we can
leverage the results to derive bounds for database queries.</p>
<p>We begin by reviewing several basic definitions in information
theory. First, the <em>entropy</em> <span
class="math inline">h(X)</span> of a finite random variable <span
class="math inline">X</span> is defined as follows:</p>
<p><span class="math display">
h(X) \stackrel{\text{def}}{=} -\sum_i p_i \log p_i
</span></p>
<p>where <span class="math inline">p_i</span> is the probability of the
outcome <span class="math inline">X_i</span>, and <span
class="math inline">\log</span> has base 2. Intuitively, the entropy
measures the <em>randomness</em> of <span class="math inline">X</span>:
if <span class="math inline">X</span> is deterministic, then:</p>
<p><span class="math display">
p_i = \begin{cases}
1 &amp; \text{when } i = c \\
0 &amp; \text{when } i \neq c
\end{cases}
</span></p>
<p>for some constant <span class="math inline">c</span>. Therefore <span
class="math inline">h(X)= - 1 \log 1 = 0</span>. If <span
class="math inline">X</span> is uniform (as random as it gets), then
<span class="math inline">h(X) = - m \times \frac{1}{m} \times \log
\frac{1}{m} = \log m</span>, where <span class="math inline">m</span> is
the size of the support for <span class="math inline">X</span>.</p>
<p>When <span class="math inline">\mathbf{X}</span> is a set of
variables, <span class="math inline">h(\mathbf{X})</span> denotes the
entropy over the joint distribution over the set. We write <span
class="math inline">h(\mathbf{X}\mathbf{Y})</span> for <span
class="math inline">h(\mathbf{X}\cup \mathbf{Y})</span>, and <span
class="math inline">h(XY)</span> for <span class="math inline">h(\{X,
Y\})</span>. For a set of <span class="math inline">n</span> random
variables <span class="math inline">\mathbf{X}</span>, we define the
<em>entropic vector</em> as a function from any subset of <span
class="math inline">\mathbf{X}</span> to the entropy of their joint
distribution:</p>
<p><span class="math display">
(h(\mathbf{X}_S))_{S \subseteq [n]} \in \mathbb{R}^{2^n}_+
</span></p>
<p>As an example, let’s consider the distribution over <span
class="math inline">X,Y,Z</span> as shown in the table below:</p>
<table>
<thead>
<tr class="header">
<th>X</th>
<th>Y</th>
<th>Z</th>
<th>p</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>1/4</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
<td>1/4</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
<td>1/4</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
<td>1/4</td>
</tr>
</tbody>
</table>
<p>The entropic vector <span class="math inline">h</span> then has
values as shown in the Hasse diagram below:</p>
<div style="text-align: center;">
<p><img title="" src="hasse.svg" alt="hasse.svg" width="350"></p>
</div>
<p>We next define the <em>conditional entropy</em> as an analog to the
conditional distribution:</p>
<p><span class="math display">
h(\mathbf{V}\mid \mathbf{U}) \stackrel{\text{def}}{=} h(\mathbf{UV}) -
h(\mathbf{U})
</span></p>
<p>Importantly, <em>the conditional entropy is not an entropic
vector</em>! In other words, there may not always be a distribution
whose entropies corresponds to a given conditional entropy. Instead, the
conditional entropy can be equivalently defined as the expectation of
the entropy of <span class="math inline">\mathbf{V}</span> given <span
class="math inline">\mathbf{U}</span>:</p>
<p><span class="math display">
h(\mathbf{V\mid U}) = \mathop{{}\mathbb{E}}_\mathbf{u}[h(\mathbf{V}\mid
\mathbf{U}=\mathbf{u})]
</span></p>
<p>The reader can verify this is indeed equivalent to the definition on
our example above.</p>
<p>One final bit of notation we need is the <em>conditional mutual
information</em>:</p>
<p><span class="math display">
I_h(\mathbf{V};\mathbf{W}\mid\mathbf{U}) \stackrel{\text{def}}{=}
h(\mathbf{UV}) + h(\mathbf{UW}) - h(\mathbf{UVW}) - h(\mathbf{U})
</span></p>
<p>The key property is that <span class="math inline">\mathbf{V}</span>
and <span class="math inline">\mathbf{W}</span> are independent given
<span class="math inline">\mathbf{U}</span> iff <span
class="math inline">I_h(\mathbf{V};\mathbf{W}\mid\mathbf{U})=0</span>.
Rearranging in terms of conditional entropies can perhaps make the
definition more intuitive:</p>
<p><span class="math display">
\begin{align*}
I_h(\mathbf{V;W \mid U})
&amp; = (h(\mathbf{UV}) - h(\mathbf{U})) - (h(\mathbf{UVW}) -
h(\mathbf{UW})) \\
&amp; = h(\mathbf{V\mid U}) - h(\mathbf{V\mid UW})
\end{align*}
</span></p>
<p>Here we see that <span class="math inline">I_h</span> measures how
much less random <span class="math inline">\mathbf{V}</span> becomes
once we know <span class="math inline">\mathbf{W}</span>, if we already
knew <span class="math inline">\mathbf{U}</span>. Or, how much more
information we gain about <span class="math inline">\mathbf{V}</span>
after conditioning on <span class="math inline">\mathbf{W}</span>. Note
that the definition is symmetric: <span
class="math inline">I_h(\mathbf{V;W\mid U}) = I_h(\mathbf{W;V\mid
U})</span>.</p>
<p>From these definitions, the three fundamental entropic inequalities,
called <em>Shannon inequalities</em>, fall out very naturally:</p>
<p><span class="math display">
\begin{align}
0 &amp; \leq h(\mathbf{U}) \leq \log |\text{supp}(\mathbf{U})| \\
0 &amp; \leq h(\mathbf{U \mid V}) \\
0 &amp; \leq I_h(\mathbf{V;W\mid U})
\end{align}
</span></p>
<p>where <span class="math inline">|\text{supp}(\mathbf{U})|</span> is
the size of the support of <span class="math inline">\mathbf{U}</span>,
i.e. number of possible outcomes. The second inequality is called
<em>monotonicity</em>, because it is equivalent to <span
class="math inline">h(\mathbf{V}) \leq h(\mathbf{UV})</span>, which says
the distribution over more variables becomes more random. The last
inequality is called <em>submodularity</em> (a.k.a. “law of diminishing
returns”). If we replace <span class="math inline">\mathbf{V}</span>
with <span class="math inline">\delta</span>, we can rewrite the
inequality as <span class="math inline">h(\mathbf{U\cup W} \cup \delta)
- h(\mathbf{U\cup W}) \leq h(\mathbf{U}\cup\delta) -
h(\mathbf{U})</span> which is exactly the law of diminishing
returns.</p>
<p>OK, enough about entropies! How can we use them for databases? The
key idea is to see a relation as a distribution over its tuples. Our
example before (copied here) assigns the same probability to every tuple
in the relation, so it defines a uniform distribution over the
tuples.</p>
<table>
<thead>
<tr class="header">
<th>X</th>
<th>Y</th>
<th>Z</th>
<th>p</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td>1/4</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>1</td>
<td>1/4</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>1</td>
<td>1/4</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>0</td>
<td>1/4</td>
</tr>
</tbody>
</table>
<p>Then, we can use the first Shannon inequality to connect the entropy
of a relation to its size, because the relation <span
class="math inline">R(X, Y, Z)</span> is exactly the support <span
class="math inline">\text{supp}(XYZ)</span>, so <span
class="math inline">h(XYZ) \leq \log |R|</span>. Similarly, we can
connect the conditional entropy to <em>degree constraints</em>. For two
sets of attributes <span class="math inline">\mathbf{U, V}</span> and
values <span class="math inline">\mathbf{u}</span>, define the
<em>degree</em> of <span class="math inline">\mathbf{V}</span> given
<span class="math inline">\mathbf{U = u}</span>, written <span
class="math inline">\text{deg}(\mathbf{V \mid U=u})</span>, as the
number of tuples with distinct <span
class="math inline">\mathbf{V}</span> values given <span
class="math inline">\mathbf{U=u}</span>. For example, <span
class="math inline">\text{deg}(XY\mid Z=0)=2</span>, <span
class="math inline">\text{deg}(X\mid YZ=01)=1</span>, and <span
class="math inline">\text{deg}(XYZ\mid \emptyset) = |R| = 4</span>. The
max degree of <span class="math inline">\mathbf{V}</span> given <span
class="math inline">\mathbf{U}</span>, written <span
class="math inline">\max \text{deg}(\mathbf{V \mid U})</span>, is the
maximum of <span class="math inline">\text{deg}(\mathbf{V \mid U =
u})</span> over all <span class="math inline">\mathbf{u \in U}</span>.
Then we have:</p>
<p><span class="math display">
\begin{align*}
h(\mathbf{V \mid U}) &amp; =
\mathop{{}\mathbb{E}}_\mathbf{u}[h(\mathbf{V}\mid
\mathbf{U}=\mathbf{u})] \\
&amp; \leq \max_\mathbf{u} h(\mathbf{V}\mid \mathbf{U}=\mathbf{u}) \\
&amp; \leq \max_\mathbf{u} \log (\text{deg}(\mathbf{V} \mid
\mathbf{U=u})) \\
&amp; = \log (\max \text{deg}(\mathbf{V\mid U}))
\end{align*}
</span></p>
<p>With this, we can translate bounds on the size of relations and
degree constraints into bounds on entropies. Note that degree
constraints generalize functional dependencies (FDs). For example an FD
<span class="math inline">X \rightarrow Y</span> can be expressed as
<span class="math inline">\text{deg}(Y\mid X) \leq 1</span>. This means
we can take advantage of functional dependencies (e.g. primary keys)
when deriving bounds on query output size. Let’s try it!</p>
<p>Consider the following query:</p>
<p><span class="math display">
Q(X, Y, Z, U) = R(X, Y) \wedge S(Y, Z) \wedge T(Z, U) \wedge A(X, Z, U)
\wedge B(X, Y, U)
</span></p>
<p>And suppose we know <span class="math inline">|R| = |S| = |T| =
N</span>, but we don’t how large <span class="math inline">A</span> and
<span class="math inline">B</span> can be. Then <span
class="math inline">Q</span> can be as large as the cartesian product of
<span class="math inline">R</span> and <span
class="math inline">T</span>, so <span class="math inline">|Q| \leq
N^2</span>. But suppose we also know the FDs <span
class="math inline">XZ \rightarrow U</span> and <span
class="math inline">YU \rightarrow X</span> hold, we can use the Shannon
inequalities to derive a tighter bound. First, consider a uniform
distribution over the <em>output</em> of <span
class="math inline">Q</span>. Then <span class="math inline">h(XY) \leq
\log |\pi_{XY}Q|\leq \log |R|</span> because the projection of the
output on <span class="math inline">XY</span> must be in the input <span
class="math inline">R(X, Y)</span>, and similarly for <span
class="math inline">S(Y, Z)</span> and <span class="math inline">T(Z,
U)</span>. By the same reasoning, <span class="math inline">h(U\mid XZ)
\leq \log \max \text{deg}_Q(U \mid XZ) \leq \log \max \text{deg}_A (U
\mid XZ)</span>, because the degree of any (sets of) attribute(s) in the
output is bounded by the degree in the input. We can now derive:</p>
<p><span class="math display">
\begin{align*}
&amp; \log |R| + \log |S| + \log |T| + \log \max \text{deg}_A (U \mid
XZ) + \log \max \text{deg}_B (X \mid YU) \\
\geq &amp; h(XY) + h(YZ) + h(ZU) + h(U\mid XZ) + h(X \mid YU) \\
\geq &amp; \underline{h(XYZ) + h(Y)} + h(ZU) + h(U\mid XZ) + h(X \mid
YU) \\
\geq &amp; h(XYZ) + \underline{h(YZU)} + h(U\mid XZ) + h(X \mid YU) \\
\geq &amp; h(XYZ) + h(YZU) + \underline{h(U\mid XYZ) + h(X \mid YZU)} \\
= &amp; 2h(XYZU) = 2 \log(|Q|)
\end{align*}
</span></p>
<p>All inequalities after the first one follow from submodularity, and
the second-to-last equality follows from the definition of conditional
entropy. Removing the <span class="math inline">\log</span> by
exponentiation on both sides, we get:</p>
<p><span class="math display">
|Q| \leq \sqrt{|R|\cdot |S| \cdot |T| \cdot \max \text{deg}(U \mid XZ)
\cdot \max \text{deg}(X\mid YU)} = N^{\frac{3}{2}}
</span></p>
<p>And it’s tighter than the naïve <span class="math inline">N^2</span>
bound!</p>
<p>Of course, to use these entropic bounds in a database system, we
can’t ask a database theorist to find a proof every time we want to run
a new query. Instead we need an automated way to compute a bound, given
a set of input sizes and degree constraints. In the next post, we will
see how to do exactly that by encoding the constraints, together with
the Shannon inequalities, into a linear program (LP). In fact, we can
even extract a proof from the LP, and use the proof to derive a query
evaluation algorithm that meets the maximum output size bound!</p>
</body>
</html>
