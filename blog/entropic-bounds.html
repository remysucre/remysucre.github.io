<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>entropic-bounds</title>
  <style>
    html {
      font-family: sans-serif;
      line-height: 1.6;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 650px;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="fundamental-entropic-bounds">Fundamental Entropic Bounds</h1>
<p>When my advisor Dan Suciu taught me entropy, he said everyone should
know 3 inequalities: the “entropy range”, monotonicity, and
submodularity. Luckily I don’t have to memorize the bounds as each
inequality has a very simple intuition. First, the “entropy range”
simply bounds the value of any entropy function:</p>
<p><span class="math display">
0 \leq H(X) \leq \log(|X|)
</span></p>
<p>On one hand, the entropy is 0 if <span class="math inline">X</span>
takes a certain value <span class="math inline">A</span> with probabilty
<span class="math inline">1</span>. In that case, we know <span
class="math inline">X</span>’s value (<span
class="math inline">A</span>) without communicating a single bit. On the
other hand, we can always simply use full binary encoding with <span
class="math inline">\log(|X|)</span> bits to encode <span
class="math inline">X</span>, ignoring the probability distribution.</p>
<p><strong>Monotonicity</strong> says that the entropy of a string of
random variables is no less than the entropy of any substring:</p>
<p><span class="math display">
H(X) \leq H(XY)
</span></p>
<p>Here <span class="math inline">XY</span> simply “concatenates” <span
class="math inline">X</span> and <span class="math inline">Y</span>, in
that a value of <span class="math inline">XY</span> concatenates a value
of <span class="math inline">X</span> with a value of <span
class="math inline">Y</span>. The entropy <span
class="math inline">H(XY)</span> is the number of bits necessary to
transmit a string in <span class="math inline">XY</span>. With this in
mind, monotonicity simpy says that transmitting more information
requires more bits.</p>
<p>Finally, our last inequality, submodularity, conveys the intuition of
“diminishing returns”: 10 dollars matter less to a millionaire than to a
PhD student. More concretely, suppose we have a function <span
class="math inline">f</span> from wealth to quality of life. Then <span
class="math inline">f</span> is submodular because <span
class="math inline">f(x + \delta) - f(x)</span> gets smaller and smaller
as <span class="math inline">x</span> increases. In the context of
information theory, <span class="math inline">H</span> is submodular
because adding additional information to a long message takes little
effort. For example, suppose a submarine needs to send reports
describing the fish it finds, and the description includes weight,
length and species. Then if it says the fish is 80 feet long, you’ll
know it’s a blue whale without looking at the species field. In general,
we can save some bits by inferring facts from the correlation of data;
if all variables are independent we can save nothing. With this
intuition, let’s look at the formal statement of
<strong>submodularity</strong>:</p>
<p><span class="math display">
H(X) + H(Y) \geq H(X \cup Y) + H(X \cap Y)
</span></p>
<p>Rearranging, we get <span class="math inline">H(X \cup Y) - H(X) \leq
H(Y) - H(X \cap Y)</span>. Note <span class="math inline">(X \cup Y) - X
= Y - (X\cap Y)</span>, and if we define <span
class="math inline">\delta = Y - (X\cap Y)</span>, the inequality
becomes <span class="math inline">H(X + \delta) - H(X) \leq H((X\cap Y)
+ \delta) - H(X\cap Y)</span>, which states precisly “the law of
diminishing returns” because <span class="math inline">X \geq (X\cap
Y)</span>!</p>
</body>
</html>
