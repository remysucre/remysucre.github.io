<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Remy Wang</title><link>https://remy.wang/posts/</link><description>Recent content in Posts on Remy Wang</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Jun 2020 00:00:00 -0700</lastBuildDate><atom:link href="https://remy.wang/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>View Images on A Remote Machine</title><link>https://remy.wang/posts/ssh-image/</link><pubDate>Thu, 18 Jun 2020 00:00:00 -0700</pubDate><guid>https://remy.wang/posts/ssh-image/</guid><description>First connect to remate machine with port-forwarding: ssh -L 8000:localhost:8000 usr@machine.cs.school.edu, cd to project directory, then serve the whole directory with python3 -m http.server &amp;amp;. Use a web browser to connect to http://0.0.0.0:8000.</description></item><item><title>Entropy</title><link>https://remy.wang/posts/entropy/</link><pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate><guid>https://remy.wang/posts/entropy/</guid><description>Entropy can be understood as the minimum amount of data needed to be transmitted in order to communicate a piece of information. Concretely, this is the average number of bits needed to encode a message. For example, imagine a spaceship sends a status code every minute to indicate if it has found any alien civilization. The code is any letter from the English alphabet, with $A$ meaning &amp;ldquo;nothing new&amp;rdquo;, and some other letter describing the alien.</description></item><item><title>Fundamental Entropic Bounds</title><link>https://remy.wang/posts/entropic-bounds/</link><pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate><guid>https://remy.wang/posts/entropic-bounds/</guid><description>When my advisor Dan Suciu taught me entropy, he said everyone should know 3 inequalities: the &amp;ldquo;entropy range&amp;rdquo;, monotonicity, and submodularity. Luckily I don&amp;rsquo;t have to memorize the bounds as each inequality has a very simple intuition. First, the &amp;ldquo;entropy range&amp;rdquo; simply bounds the value of any entropy function:
0 \leq H(X) \leq \log(|X|) On one hand, the entropy is 0 if $X$ takes a certain value $A$ with probabilty $1$.</description></item></channel></rss>