<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>information-theory on Remy Wang</title><link>https://remywang.science/tags/information-theory/</link><description>Recent content in information-theory on Remy Wang</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 17 Jun 2020 00:00:00 -0700</lastBuildDate><atom:link href="https://remywang.science/tags/information-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>Entropy</title><link>https://remywang.science/posts/entropy/</link><pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate><guid>https://remywang.science/posts/entropy/</guid><description>Entropy can be understood as the minimum amount of data needed to be transmitted in order to communicate a piece of information. Concretely, this is the average number of bits needed to encode a message. For example, imagine a spaceship sends a status code every minute to indicate if it has found any alien civilization. The code is any letter from the English alphabet, with $A$ meaning &amp;ldquo;nothing new&amp;rdquo;, and some other letter describing the alien.</description></item><item><title>Fundamental Entropic Bounds</title><link>https://remywang.science/posts/entropic-bounds/</link><pubDate>Wed, 17 Jun 2020 00:00:00 -0700</pubDate><guid>https://remywang.science/posts/entropic-bounds/</guid><description>When my advisor Dan Suciu taught me entropy, he said everyone should know 3 inequalities: the &amp;ldquo;entropy range&amp;rdquo;, monotonicity, and submodularity. Luckily I don&amp;rsquo;t have to memorize the bounds as each inequality has a very simple intuition. First, the &amp;ldquo;entropy range&amp;rdquo; simply bounds the value of any entropy function:
0 \leq H(X) \leq \log(|X|) On one hand, the entropy is 0 if $X$ takes a certain value $A$ with probabilty $1$.</description></item></channel></rss>